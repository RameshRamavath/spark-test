# PROD CHICAGO
# Add one more channel below as we are testing it to two data centers
tier1.sources = tibco_blaze_reston tibco_blaze_reston2 tibco_blaze_chicago tibco_blaze_chicago2
tier1.channels = kafka_transaction_channel_rs kafka_transaction_channel_ch file_transaction_channel
tier1.sinks = hdfs_sink_ch
#=============#=============================QueueName=============================#
#       Tibco     #          WU.PROD.BLAZE.HADOOP.XML.PACKET.QUEUE           #
#=================#===================================================================#
# kafka_transaction_channel_ch
tier1.channels.kafka_transaction_channel_ch.type = org.apache.flume.channel.kafka.KafkaChannel
tier1.channels.kafka_transaction_channel_ch.kafka.bootstrap.servers = 10.47.27.140:9092,10.47.27.160:9092,10.47.27.180:9092
tier1.channels.kafka_transaction_channel_ch.zookeeperConnect = 10.47.27.140:2181,10.47.27.160:2181,10.47.27.180:2181
tier1.channels.kafka_transaction_channel_ch.kafka.topic = blaze_transaction_all
tier1.channels.kafka_transaction_channel_ch.kafka.fetch.message.max.bytes = 6608000
tier1.channels.kafka_transaction_channel_ch.kafka.max.partition.fetch.bytes = 6608000
tier1.channels.kafka_transaction_channel_ch.transactioncapacity = 100000
tier1.channels.kafka_transaction_channel_ch.capacity = 10000000
tier1.channels.kafka_transaction_channel_ch.kafka.max.request.size = 6608000
tier1.channels.kafka_transaction_channel_ch.kafka.message.max.bytes = 6608000
tier1.channels.kafka_transaction_channel_ch.kafka.consumer.fetch.message.max.bytes = 6608000
tier1.channels.kafka_transaction_channel_ch.kafka.consumer.max.partition.fetch.bytes = 6608000
tier1.channels.kafka_transaction_channel_ch.kafka.consumer.message.max.bytes = 6608000
tier1.channels.kafka_transaction_channel_ch.consumer.message.max.bytes = 6608000
tier1.channels.kafka_transaction_channel_ch.consumer.max.request.size = 6608000
tier1.channels.kafka_transaction_channel_ch.kafka.producer.max.request.size = 6608000
tier1.channels.kafka_transaction_channel_ch.kafka.producer.compression.type = snappy
tier1.channels.kafka_transaction_channel_ch.kafka.producer.acks = 1
tier1.channels.kafka_transaction_channel_ch.kafka.batch.size = 1
tier1.channels.kafka_transaction_channel_ch.kafka.producer.batch.size = 1
# kafka_transaction_channel_rs
tier1.channels.kafka_transaction_channel_rs.type = org.apache.flume.channel.kafka.KafkaChannel
tier1.channels.kafka_transaction_channel_rs.kafka.bootstrap.servers = wuwvc9hdbdt40:9092,wuwvc9hdbdt60:9092,wuwvc9hdbdt80:9092
tier1.channels.kafka_transaction_channel_rs.zookeeperConnect = wuwvc9hdbdt01:2181,wuwvc9hdbdt31:2181,wuwvc9hdbdt71:2181
tier1.channels.kafka_transaction_channel_rs.kafka.topic = blaze_transaction_all
tier1.channels.kafka_transaction_channel_rs.kafka.fetch.message.max.bytes = 6608000
tier1.channels.kafka_transaction_channel_rs.kafka.max.partition.fetch.bytes = 6608000
tier1.channels.kafka_transaction_channel_rs.transactioncapacity = 100000
tier1.channels.kafka_transaction_channel_rs.capacity = 10000000
tier1.channels.kafka_transaction_channel_rs.kafka.max.request.size = 6608000
tier1.channels.kafka_transaction_channel_rs.kafka.message.max.bytes = 6608000
tier1.channels.kafka_transaction_channel_rs.kafka.consumer.fetch.message.max.bytes = 6608000
tier1.channels.kafka_transaction_channel_rs.kafka.consumer.max.partition.fetch.bytes = 6608000
tier1.channels.kafka_transaction_channel_rs.kafka.consumer.message.max.bytes = 6608000
tier1.channels.kafka_transaction_channel_rs.consumer.message.max.bytes = 6608000
tier1.channels.kafka_transaction_channel_rs.consumer.max.request.size = 6608000
tier1.channels.kafka_transaction_channel_rs.kafka.producer.max.request.size = 6608000
tier1.channels.kafka_transaction_channel_rs.kafka.producer.compression.type = snappy
tier1.channels.kafka_transaction_channel_rs.kafka.producer.acks = 1
tier1.channels.kafka_transaction_channel_rs.kafka.batch.size = 1
tier1.channels.kafka_transaction_channel_rs.kafka.producer.batch.size = 1
#File Channel , to workaround hdfs sink issue in kafka client max message size
tier1.channels.file_transaction_channel.type = file
tier1.channels.file_transaction_channel.checkpointDir = /var/lib/flume-ng/transaction_channel/checkpoint
tier1.channels.file_transaction_channel.dataDirs = /var/lib/flume-ng/transaction_channel/data
tier1.channels.file_transaction_channel.transactioncapacity = 100000
tier1.channels.file_transaction_channel.capacity = 10000000
#source -- Reston 1
tier1.sources.tibco_blaze_reston.type = jms
tier1.sources.tibco_blaze_reston.selector.type = replicating
tier1.sources.tibco_blaze_reston.channels = kafka_transaction_channel_ch kafka_transaction_channel_rs file_transaction_channel
tier1.sources.tibco_blaze_reston.initialContextFactory = com.tibco.tibjms.naming.SSLTibjmsInitialContextFactory
tier1.sources.tibco_blaze_reston.connectionFactory = SSLQueueConnectionFactory
tier1.sources.tibco_blaze_reston.converter.type = com.wu.flume.converter.WUJMSMessageConverter
tier1.sources.tibco_blaze_reston.providerURL = ssl://10.45.17.72:7343,ssl://10.45.17.78:7343
tier1.sources.tibco_blaze_reston.destinationName = WU.PROD.BLAZE.HADOOP.XML.PACKET.QUEUE
tier1.sources.tibco_blaze_reston.destinationType = QUEUE
tier1.sources.tibco_blaze_reston.userName = hadoopUser
tier1.sources.tibco_blaze_reston.passwordFile = ******
tier1.sources.tibco_blaze_reston.batchSize=1
tier1.sources.tibco_blaze_reston.interceptors = i1 i2 i3 i4
tier1.sources.tibco_blaze_reston.interceptors.i1.type = org.apache.flume.interceptor.HostInterceptor$Builder
tier1.sources.tibco_blaze_reston.interceptors.i1.preserveExisting = false
tier1.sources.tibco_blaze_reston.interceptors.i1.hostHeader = hostname
tier1.sources.tibco_blaze_reston.interceptors.i2.type = org.apache.flume.interceptor.TimestampInterceptor$Builder
tier1.sources.tibco_blaze_reston.interceptors.i2.header = flumeTimestamp
tier1.sources.tibco_blaze_reston.interceptors.i3.type = org.apache.flume.interceptor.StaticInterceptor$Builder
tier1.sources.tibco_blaze_reston.interceptors.i3.key = ******
tier1.sources.tibco_blaze_reston.interceptors.i3.value = reston
tier1.sources.tibco_blaze_reston.interceptors.i4.type = com.wu.flume.interceptor.HeaderCopyInterceptor$Builder
tier1.sources.tibco_blaze_reston.interceptors.i4.fromKey = ******
tier1.sources.tibco_blaze_reston.interceptors.i4.toKey = ******
tier1.sources.tibco_blaze_reston.interceptors.i4.ignoreValues = 1213109999999997
#source -- Reston 2
tier1.sources.tibco_blaze_reston2.type = jms
tier1.sources.tibco_blaze_reston2.selector.type = replicating
tier1.sources.tibco_blaze_reston2.channels = kafka_transaction_channel_ch kafka_transaction_channel_rs file_transaction_channel
tier1.sources.tibco_blaze_reston2.initialContextFactory = com.tibco.tibjms.naming.SSLTibjmsInitialContextFactory
tier1.sources.tibco_blaze_reston2.connectionFactory = SSLQueueConnectionFactory
tier1.sources.tibco_blaze_reston2.converter.type = com.wu.flume.converter.WUJMSMessageConverter
tier1.sources.tibco_blaze_reston2.providerURL = ssl://10.45.17.72:7343,ssl://10.45.17.78:7343
tier1.sources.tibco_blaze_reston2.destinationName = WU.PROD.BLAZE.HADOOP.XML.PACKET.QUEUE
tier1.sources.tibco_blaze_reston2.destinationType = QUEUE
tier1.sources.tibco_blaze_reston2.userName = hadoopUser
tier1.sources.tibco_blaze_reston2.passwordFile = ******
tier1.sources.tibco_blaze_reston2.batchSize=1
tier1.sources.tibco_blaze_reston2.interceptors = i1 i2 i3 i4
tier1.sources.tibco_blaze_reston2.interceptors.i1.type = org.apache.flume.interceptor.HostInterceptor$Builder
tier1.sources.tibco_blaze_reston2.interceptors.i1.preserveExisting = false
tier1.sources.tibco_blaze_reston2.interceptors.i1.hostHeader = hostname
tier1.sources.tibco_blaze_reston2.interceptors.i2.type = org.apache.flume.interceptor.TimestampInterceptor$Builder
tier1.sources.tibco_blaze_reston2.interceptors.i2.header = flumeTimestamp
tier1.sources.tibco_blaze_reston2.interceptors.i3.type = org.apache.flume.interceptor.StaticInterceptor$Builder
tier1.sources.tibco_blaze_reston2.interceptors.i3.key = ******
tier1.sources.tibco_blaze_reston2.interceptors.i3.value = reston
tier1.sources.tibco_blaze_reston2.interceptors.i4.type = com.wu.flume.interceptor.HeaderCopyInterceptor$Builder
tier1.sources.tibco_blaze_reston2.interceptors.i4.fromKey = ******
tier1.sources.tibco_blaze_reston2.interceptors.i4.toKey = ******
tier1.sources.tibco_blaze_reston2.interceptors.i4.ignoreValues = 1213109999999997,9999999997
# source -- Chicago 1
#tier1.sources.tibco_blaze_chicago.providerURL = ssl://10.47.16.217:7343,ssl://10.47.16.217:7343
tier1.sources.tibco_blaze_chicago.type = jms
tier1.sources.tibco_blaze_chicago.selector.type = replicating
tier1.sources.tibco_blaze_chicago.channels = kafka_transaction_channel_ch kafka_transaction_channel_rs file_transaction_channel
tier1.sources.tibco_blaze_chicago.initialContextFactory = com.tibco.tibjms.naming.SSLTibjmsInitialContextFactory
#tier1.sources.tibco_blaze_chicago.connectionFactory = SSLQueueConnectionFactory
tier1.sources.tibco_blaze_chicago.connectionFactory = FTQueueConnectionFactory
tier1.sources.tibco_blaze_chicago.converter.type = com.wu.flume.converter.WUJMSMessageConverter
#tier1.sources.tibco_blaze_chicago.providerURL = ssl://10.47.17.72:7343,ssl://10.47.17.78:7343
tier1.sources.tibco_blaze_chicago.providerURL = tcp://10.47.17.72:7343,tcp://10.47.17.78:7343
tier1.sources.tibco_blaze_chicago.destinationName = WU.PROD.BLAZE.HADOOP.XML.PACKET.QUEUE
tier1.sources.tibco_blaze_chicago.destinationType = QUEUE
tier1.sources.tibco_blaze_chicago.userName = hadoopUser
tier1.sources.tibco_blaze_chicago.passwordFile = ******
tier1.sources.tibco_blaze_chicago.batchSize=1
tier1.sources.tibco_blaze_chicago.interceptors = i1 i2 i3 i4
tier1.sources.tibco_blaze_chicago.interceptors.i1.type = org.apache.flume.interceptor.HostInterceptor$Builder
tier1.sources.tibco_blaze_chicago.interceptors.i1.preserveExisting = false
tier1.sources.tibco_blaze_chicago.interceptors.i1.hostHeader = hostname
tier1.sources.tibco_blaze_chicago.interceptors.i2.type = org.apache.flume.interceptor.TimestampInterceptor$Builder
tier1.sources.tibco_blaze_chicago.interceptors.i2.header = flumeTimestamp
tier1.sources.tibco_blaze_chicago.interceptors.i3.type = org.apache.flume.interceptor.StaticInterceptor$Builder
tier1.sources.tibco_blaze_chicago.interceptors.i3.key = ******
tier1.sources.tibco_blaze_chicago.interceptors.i3.value = chicago
tier1.sources.tibco_blaze_chicago.interceptors.i4.type = com.wu.flume.interceptor.HeaderCopyInterceptor$Builder
tier1.sources.tibco_blaze_chicago.interceptors.i4.fromKey = ******
tier1.sources.tibco_blaze_chicago.interceptors.i4.toKey = ******
tier1.sources.tibco_blaze_chicago.interceptors.i4.ignoreValues = 1213109999999997
# source -- Chicago 2
tier1.sources.tibco_blaze_chicago2.type = jms
tier1.sources.tibco_blaze_chicago2.selector.type = replicating
tier1.sources.tibco_blaze_chicago2.channels = kafka_transaction_channel_ch kafka_transaction_channel_rs file_transaction_channel
tier1.sources.tibco_blaze_chicago2.initialContextFactory = com.tibco.tibjms.naming.SSLTibjmsInitialContextFactory
#tier1.sources.tibco_blaze_chicago2.connectionFactory = SSLQueueConnectionFactory
tier1.sources.tibco_blaze_chicago2.connectionFactory = FTQueueConnectionFactory
tier1.sources.tibco_blaze_chicago2.converter.type = com.wu.flume.converter.WUJMSMessageConverter
#tier1.sources.tibco_blaze_chicago2.providerURL = ssl://10.47.17.72:7343,ssl://10.47.17.78:7343
tier1.sources.tibco_blaze_chicago2.providerURL = tcp://10.47.17.72:7343,tcp://10.47.17.78:7343
tier1.sources.tibco_blaze_chicago2.destinationName = WU.PROD.BLAZE.HADOOP.XML.PACKET.QUEUE
tier1.sources.tibco_blaze_chicago2.destinationType = QUEUE
tier1.sources.tibco_blaze_chicago2.userName = hadoopUser
tier1.sources.tibco_blaze_chicago2.passwordFile = ******
tier1.sources.tibco_blaze_chicago2.batchSize=1
tier1.sources.tibco_blaze_chicago2.interceptors = i1 i2 i3 i4
tier1.sources.tibco_blaze_chicago2.interceptors.i1.type = org.apache.flume.interceptor.HostInterceptor$Builder
tier1.sources.tibco_blaze_chicago2.interceptors.i1.preserveExisting = false
tier1.sources.tibco_blaze_chicago2.interceptors.i1.hostHeader = hostname
tier1.sources.tibco_blaze_chicago2.interceptors.i2.type = org.apache.flume.interceptor.TimestampInterceptor$Builder
tier1.sources.tibco_blaze_chicago2.interceptors.i2.header = flumeTimestamp
tier1.sources.tibco_blaze_chicago2.interceptors.i3.type = org.apache.flume.interceptor.StaticInterceptor$Builder
tier1.sources.tibco_blaze_chicago2.interceptors.i3.key = ******
tier1.sources.tibco_blaze_chicago2.interceptors.i3.value = chicago
tier1.sources.tibco_blaze_chicago2.interceptors.i4.type = com.wu.flume.interceptor.HeaderCopyInterceptor$Builder
tier1.sources.tibco_blaze_chicago2.interceptors.i4.fromKey = ******
tier1.sources.tibco_blaze_chicago2.interceptors.i4.toKey = ******
tier1.sources.tibco_blaze_chicago2.interceptors.i4.ignoreValues = 1213109999999997
# sink -- Chicago
tier1.sinks.hdfs_sink_ch.type = hdfs
tier1.sinks.hdfs_sink_ch.channel = file_transaction_channel
tier1.sinks.hdfs_sink_ch.hdfs.path = /user/bdas/warehouse/kafka/blaze_transaction_all/ds=%Y%m%d/hr=%H/mn=%M
tier1.sinks.hdfs_sink_ch.hdfs.fileType = CompressedStream
tier1.sinks.hdfs_sink_ch.hdfs.codeC = gzip
tier1.sinks.hdfs_sink_ch.hdfs.inUsePrefix = _
tier1.sinks.hdfs_sink_ch.hdfs.filePrefix = %{datacenter}.%{hostname}
tier1.sinks.hdfs_sink_ch.hdfs.fileSuffix = .txt.gz
tier1.sinks.hdfs_sink_ch.hdfs.threadsPoolSize = 20
tier1.sinks.hdfs_sink_ch.hdfs.idleTimeout = 300
tier1.sinks.hdfs_sink_ch.hdfs.batchSize = 100
tier1.sinks.hdfs_sink_ch.hdfs.rollInterval = 300
tier1.sinks.hdfs_sink_ch.hdfs.rollSize = 0
tier1.sinks.hdfs_sink_ch.hdfs.rollCount = 0
tier1.sinks.hdfs_sink_ch.hdfs.maxOpenFiles = 100
tier1.sinks.hdfs_sink_ch.hdfs.round = true
tier1.sinks.hdfs_sink_ch.hdfs.roundValue = 10
tier1.sinks.hdfs_sink_ch.hdfs.roundUnit = minute
tier1.sinks.hdfs_sink_ch.hdfs.useLocalTimeStamp = true
tier1.sinks.hdfs_sink_ch.hdfs.callTimeout = 80000
tier1.sinks.hdfs_sink_ch.serializer = com.wu.flume.serialization.HeaderAndBodyTextEventSerializer$Builder
tier1.sinks.hdfs_sink_ch.serializer.columns = NONE
tier1.sinks.hdfs_sink_ch.serializer.format = FLATCSV
tier1.sinks.hdfs_sink_ch.serializer.appendNewline = true
tier1.sinks.hdfs_sink_ch.serializer.field.delim = '\t'
tier1.sinks.hdfs_sink_ch.serializer.line.delim = '\n'
tier1.sinks.hdfs_sink_ch.hdfs.kerberosPrincipal = flume@CDH.PROD.WUDIP.COM
tier1.sinks.hdfs_sink_ch.hdfs.kerberosKeytab = ******